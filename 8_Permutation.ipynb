{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ea0ce5-bae1-440f-9401-5288b293a405",
   "metadata": {},
   "source": [
    "## PermÃ¼tasyon Based Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a1a70b-70d7-4c28-bb01-8b228fcc5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Layer\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd2af74-60d3-4f35-88fe-e0a09fb33873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPU(s). GPU acceleration enabled.\n"
     ]
    }
   ],
   "source": [
    "# Enable GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Found {len(gpus)} GPU(s). GPU acceleration enabled.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found. Running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d442327-2b19-4e06-901f-b6356fbb46e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output directory: feature_importance_analysis_20250620_232827\n"
     ]
    }
   ],
   "source": [
    "# Create output directory for visualizations\n",
    "output_dir = f\"feature_importance_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"Created output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f691cca-930b-439d-8466-1460b2e619a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Self-Attention Layer definition (same as training)\n",
    "class SelfAttention(Layer):\n",
    "    def __init__(self, attention_units=128, return_attention=False, **kwargs):\n",
    "        self.attention_units = attention_units\n",
    "        self.return_attention = return_attention\n",
    "        super(SelfAttention, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.time_steps = input_shape[1]\n",
    "        self.input_dim = input_shape[2]\n",
    "        \n",
    "        self.query_dense = tf.keras.layers.Dense(self.attention_units)\n",
    "        self.key_dense = tf.keras.layers.Dense(self.attention_units)\n",
    "        self.value_dense = tf.keras.layers.Dense(self.input_dim)\n",
    "        self.context_dense = tf.keras.layers.Dense(self.input_dim)\n",
    "        \n",
    "        super(SelfAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        \n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        score = score / tf.math.sqrt(tf.cast(self.attention_units, tf.float32))\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "        context = tf.matmul(attention_weights, value)\n",
    "        output = self.context_dense(context)\n",
    "        \n",
    "        if self.return_attention:\n",
    "            return output, attention_weights\n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[1], self.input_dim), \n",
    "                    (input_shape[0], input_shape[1], input_shape[1])]\n",
    "        return (input_shape[0], input_shape[1], self.input_dim)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(SelfAttention, self).get_config()\n",
    "        config.update({\n",
    "            'attention_units': self.attention_units,\n",
    "            'return_attention': self.return_attention\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94f3134-166d-4798-b643-616c52cee947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cost_metric(y_true, y_pred):\n",
    "    y_pred = tf.nn.softmax(y_pred)\n",
    "    pred_class = tf.argmax(y_pred, axis=1)\n",
    "    true_class = tf.argmax(y_true, axis=1)\n",
    "    \n",
    "    cost_matrix = tf.constant([\n",
    "        [0, 7, 8, 9, 10],\n",
    "        [200, 0, 7, 8, 9],\n",
    "        [300, 200, 0, 7, 8],\n",
    "        [400, 300, 200, 0, 7],\n",
    "        [500, 400, 300, 200, 0]\n",
    "    ], dtype=tf.float32)\n",
    "    \n",
    "    costs = tf.gather_nd(cost_matrix, tf.stack([true_class, pred_class], axis=1))\n",
    "    return tf.reduce_mean(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f7ffcc-6a87-4302-819c-a9b391077546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: trained_model_bilstm_attention_deneme_25_80296_27/bilstm_attention_model_epoch_27.keras\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"trained_model_bilstm_attention_deneme_25_80296_27\"\n",
    "\n",
    "# Register custom objects\n",
    "custom_objects = {\n",
    "    'SelfAttention': SelfAttention,\n",
    "    'custom_cost_metric': custom_cost_metric\n",
    "}\n",
    "\n",
    "# Load model\n",
    "epoch_to_load = 27\n",
    "model_path = f\"{model_dir}/bilstm_attention_model_epoch_{epoch_to_load:02d}.keras\"\n",
    "\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "model = load_model(model_path, custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cd2b459-d359-4d31-b285-db249427512f",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = joblib.load(f\"{model_dir}/label_encoder.joblib\")\n",
    "scaler = joblib.load(f\"{model_dir}/standard_scaler.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfbf2f13-cc8a-4ad8-8d04-32d24d9fc06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_operational = pd.read_csv('selected_and_filled_validation_operational_readouts.csv')\n",
    "validation_labels = pd.read_csv('validation_labels.csv')\n",
    "\n",
    "# Get feature names\n",
    "feature_names = validation_operational.columns[2:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaa025c7-ea70-4761-af8e-704c7d570b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_data():\n",
    "    \"\"\"Prepare all validation data for analysis\"\"\"\n",
    "    vehicle_groups = validation_operational.groupby('vehicle_id')\n",
    "    vehicle_ids = list(vehicle_groups.groups.keys())\n",
    "    \n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    vehicle_id_list = []\n",
    "    \n",
    "    print(f\"Processing {len(vehicle_ids)} vehicles...\")\n",
    "    \n",
    "    for i, vehicle_id in enumerate(vehicle_ids):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing vehicle {i}/{len(vehicle_ids)}\")\n",
    "            \n",
    "        group = vehicle_groups.get_group(vehicle_id)\n",
    "        time_series = group.sort_values('time_step').iloc[:, 2:].values\n",
    "        \n",
    "        # Scale the data\n",
    "        time_series_scaled = scaler.transform(time_series)\n",
    "        \n",
    "        # Get label\n",
    "        label_row = validation_labels[validation_labels['vehicle_id'] == vehicle_id]\n",
    "        if len(label_row) > 0:\n",
    "            label = label_row['class_label'].values[0]\n",
    "            encoded_label = le.transform([label])[0]\n",
    "            \n",
    "            X_data.append(time_series_scaled)\n",
    "            y_data.append(encoded_label)\n",
    "            vehicle_id_list.append(vehicle_id)\n",
    "    \n",
    "    # Pad sequences to same length\n",
    "    max_length = max(len(seq) for seq in X_data)\n",
    "    X_padded = pad_sequences(X_data, maxlen=max_length, padding='post', dtype='float32')\n",
    "    \n",
    "    print(f\"Total samples prepared: {len(X_padded)}\")\n",
    "    print(f\"Sequence length: {max_length}\")\n",
    "    print(f\"Number of features: {X_padded.shape[2]}\")\n",
    "    \n",
    "    return X_padded, np.array(y_data), vehicle_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddc1086b-859d-4500-989c-3aa2368b14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU-accelerated permutation importance\n",
    "@tf.function\n",
    "def compute_predictions_gpu(model, inputs):\n",
    "    \"\"\"Compute predictions on GPU\"\"\"\n",
    "    return model(inputs, training=False)\n",
    "\n",
    "def compute_permutation_importance_gpu(model, X_data, y_data, batch_size=128, n_permutations=3, sample_size=None):\n",
    "    \"\"\"\n",
    "    GPU-accelerated permutation importance computation\n",
    "    \"\"\"\n",
    "    n_samples = len(X_data)\n",
    "    n_features = X_data.shape[2]\n",
    "    n_classes = len(le.classes_)\n",
    "    \n",
    "    if sample_size and sample_size < n_samples:\n",
    "        indices = np.random.choice(n_samples, sample_size, replace=False)\n",
    "        X_data = X_data[indices]\n",
    "        y_data = y_data[indices]\n",
    "        n_samples = sample_size\n",
    "        print(f\"Using {n_samples} samples for feature importance computation\")\n",
    "    \n",
    "    # Store importance scores\n",
    "    importance_scores = np.zeros((n_classes, n_features))\n",
    "    \n",
    "    print(f\"\\nComputing GPU-accelerated permutation importance...\")\n",
    "    print(f\"Batch size: {batch_size}, Permutations: {n_permutations}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Convert to TensorFlow dataset for efficient batching\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X_data)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    # Get baseline predictions\n",
    "    baseline_probs = []\n",
    "    for batch in dataset:\n",
    "        with tf.device('/GPU:0'):\n",
    "            batch_pred = compute_predictions_gpu(model, batch)\n",
    "            batch_prob = tf.nn.softmax(batch_pred)\n",
    "            baseline_probs.append(batch_prob.numpy())\n",
    "    baseline_probs = np.vstack(baseline_probs)\n",
    "    \n",
    "    # Process each feature\n",
    "    for feature_idx in range(n_features):\n",
    "        if feature_idx % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Feature {feature_idx}/{n_features} - Elapsed: {elapsed:.1f}s\")\n",
    "        \n",
    "        feature_importance = np.zeros(n_classes)\n",
    "        \n",
    "        # Run multiple permutations\n",
    "        for perm in range(n_permutations):\n",
    "            # Create permuted data\n",
    "            X_permuted = X_data.copy()\n",
    "            \n",
    "            # Permute the feature\n",
    "            for i in range(n_samples):\n",
    "                perm_values = X_permuted[i, :, feature_idx].copy()\n",
    "                np.random.shuffle(perm_values)\n",
    "                X_permuted[i, :, feature_idx] = perm_values\n",
    "            \n",
    "            # Create dataset for permuted data\n",
    "            perm_dataset = tf.data.Dataset.from_tensor_slices(X_permuted)\n",
    "            perm_dataset = perm_dataset.batch(batch_size)\n",
    "            \n",
    "            # Get predictions with permuted feature\n",
    "            permuted_probs = []\n",
    "            for batch in perm_dataset:\n",
    "                with tf.device('/GPU:0'):\n",
    "                    batch_pred = compute_predictions_gpu(model, batch)\n",
    "                    batch_prob = tf.nn.softmax(batch_pred)\n",
    "                    permuted_probs.append(batch_prob.numpy())\n",
    "            permuted_probs = np.vstack(permuted_probs)\n",
    "            \n",
    "            # Calculate importance for each class\n",
    "            for class_idx in range(n_classes):\n",
    "                importance = np.mean(np.abs(baseline_probs[:, class_idx] - permuted_probs[:, class_idx]))\n",
    "                feature_importance[class_idx] += importance\n",
    "        \n",
    "        # Average over permutations\n",
    "        importance_scores[:, feature_idx] = feature_importance / n_permutations\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Feature importance computation completed in {total_time:.1f} seconds\")\n",
    "    \n",
    "    return importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "229d19f4-7047-4d39-9bea-4c49edbcc8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing all validation data for analysis...\n",
      "Processing 5046 vehicles...\n",
      "Processing vehicle 0/5046\n",
      "Processing vehicle 100/5046\n",
      "Processing vehicle 200/5046\n",
      "Processing vehicle 300/5046\n",
      "Processing vehicle 400/5046\n",
      "Processing vehicle 500/5046\n",
      "Processing vehicle 600/5046\n",
      "Processing vehicle 700/5046\n",
      "Processing vehicle 800/5046\n",
      "Processing vehicle 900/5046\n",
      "Processing vehicle 1000/5046\n",
      "Processing vehicle 1100/5046\n",
      "Processing vehicle 1200/5046\n",
      "Processing vehicle 1300/5046\n",
      "Processing vehicle 1400/5046\n",
      "Processing vehicle 1500/5046\n",
      "Processing vehicle 1600/5046\n",
      "Processing vehicle 1700/5046\n",
      "Processing vehicle 1800/5046\n",
      "Processing vehicle 1900/5046\n",
      "Processing vehicle 2000/5046\n",
      "Processing vehicle 2100/5046\n",
      "Processing vehicle 2200/5046\n",
      "Processing vehicle 2300/5046\n",
      "Processing vehicle 2400/5046\n",
      "Processing vehicle 2500/5046\n",
      "Processing vehicle 2600/5046\n",
      "Processing vehicle 2700/5046\n",
      "Processing vehicle 2800/5046\n",
      "Processing vehicle 2900/5046\n",
      "Processing vehicle 3000/5046\n",
      "Processing vehicle 3100/5046\n",
      "Processing vehicle 3200/5046\n",
      "Processing vehicle 3300/5046\n",
      "Processing vehicle 3400/5046\n",
      "Processing vehicle 3500/5046\n",
      "Processing vehicle 3600/5046\n",
      "Processing vehicle 3700/5046\n",
      "Processing vehicle 3800/5046\n",
      "Processing vehicle 3900/5046\n",
      "Processing vehicle 4000/5046\n",
      "Processing vehicle 4100/5046\n",
      "Processing vehicle 4200/5046\n",
      "Processing vehicle 4300/5046\n",
      "Processing vehicle 4400/5046\n",
      "Processing vehicle 4500/5046\n",
      "Processing vehicle 4600/5046\n",
      "Processing vehicle 4700/5046\n",
      "Processing vehicle 4800/5046\n",
      "Processing vehicle 4900/5046\n",
      "Processing vehicle 5000/5046\n",
      "Total samples prepared: 5046\n",
      "Sequence length: 209\n",
      "Number of features: 61\n",
      "\n",
      "Class distribution in data:\n",
      "  Class 0: 4910 samples\n",
      "  Class 1: 16 samples\n",
      "  Class 2: 14 samples\n",
      "  Class 3: 30 samples\n",
      "  Class 4: 76 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing all validation data for analysis...\")\n",
    "X_data, y_data, vehicle_ids = prepare_all_data()\n",
    "\n",
    "# Check class distribution\n",
    "unique_classes, class_counts = np.unique(y_data, return_counts=True)\n",
    "print(f\"\\nClass distribution in data:\")\n",
    "for cls, count in zip(unique_classes, class_counts):\n",
    "    class_label = le.inverse_transform([cls])[0]\n",
    "    print(f\"  Class {class_label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35675bd5-f7f5-477d-b033-434f624b70c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2000 samples for feature importance computation\n",
      "\n",
      "Computing GPU-accelerated permutation importance...\n",
      "Batch size: 128, Permutations: 3\n",
      "Feature 0/61 - Elapsed: 7.0s\n",
      "Feature 10/61 - Elapsed: 45.2s\n",
      "Feature 20/61 - Elapsed: 84.1s\n",
      "Feature 30/61 - Elapsed: 124.1s\n",
      "Feature 40/61 - Elapsed: 166.5s\n",
      "Feature 50/61 - Elapsed: 208.4s\n",
      "Feature 60/61 - Elapsed: 251.6s\n",
      "Feature importance computation completed in 255.9 seconds\n"
     ]
    }
   ],
   "source": [
    "# Compute feature importance using GPU acceleration\n",
    "# Use sample_size to limit computation time\n",
    "importance_scores = compute_permutation_importance_gpu(\n",
    "    model, X_data, y_data, \n",
    "    batch_size=128, \n",
    "    n_permutations=3,  \n",
    "    sample_size=min(2000, len(X_data))  # Use up to 2000 samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "073da0a6-f680-490a-b88d-0d67521a5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to dictionary format\n",
    "importance_dict = {}\n",
    "for i in range(len(le.classes_)):\n",
    "    if i in unique_classes:\n",
    "        importance_dict[le.classes_[i]] = importance_scores[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e937d4e4-4919-44ed-90c8-8237df72d9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating visualizations...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCreating visualizations...\")\n",
    "\n",
    "# Visualization 1: Feature importance heatmap by class\n",
    "plt.figure(figsize=(14, 8))\n",
    "importance_df = pd.DataFrame(importance_dict, index=feature_names)\n",
    "importance_df = importance_df.T\n",
    "\n",
    "# Sort features by mean importance\n",
    "mean_importance = importance_df.mean(axis=0)\n",
    "top_features_idx = mean_importance.nlargest(30).index\n",
    "importance_df_top = importance_df[top_features_idx]\n",
    "\n",
    "sns.heatmap(importance_df_top, cmap='YlOrRd', cbar_kws={'label': 'Importance Score'}, \n",
    "            vmin=0, center=importance_df_top.mean().mean())\n",
    "plt.title('Feature Importance Heatmap by Class (Top 30 Features)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Classes')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'permutation_importance_heatmap.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97bc1f22-31db-4479-9b2d-51b94d5e5588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Top features bar plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_20_features = mean_importance.nlargest(20)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(top_20_features)))\n",
    "bars = plt.barh(top_20_features.index, top_20_features.values, color=colors)\n",
    "plt.xlabel('Mean Importance Score')\n",
    "plt.title('Top 20 Most Important Features (GPU-Accelerated Permutation)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'permutation_feature_importance_bar.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63965617-e30d-4dbb-a156-8873dc1ac1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 3: Class-specific feature importance\n",
    "n_classes_present = len(unique_classes)\n",
    "fig, axes = plt.subplots((n_classes_present + 1) // 2, 2, figsize=(15, 4 * ((n_classes_present + 1) // 2)))\n",
    "if n_classes_present == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "for idx, class_idx in enumerate(unique_classes):\n",
    "    if idx < len(axes):\n",
    "        ax = axes[idx]\n",
    "        class_name = le.inverse_transform([class_idx])[0]\n",
    "        if class_name in importance_df.index:\n",
    "            class_importance = importance_df.loc[class_name].nlargest(15)\n",
    "            colors = plt.cm.plasma(np.linspace(0, 1, len(class_importance)))\n",
    "            ax.barh(class_importance.index, class_importance.values, color=colors)\n",
    "            ax.set_xlabel('Importance Score')\n",
    "            ax.set_title(f'Top 15 Features for Class: {class_name}')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide extra subplots\n",
    "for idx in range(n_classes_present, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'class_specific_feature_importance.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4fc6365-fcc3-4045-aa2e-2484d7718e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 4: Feature importance variance\n",
    "plt.figure(figsize=(12, 6))\n",
    "feature_variance = importance_df.var(axis=0).nlargest(20)\n",
    "plt.bar(range(len(feature_variance)), feature_variance.values, color='coral')\n",
    "plt.xticks(range(len(feature_variance)), feature_variance.index, rotation=45, ha='right')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Variance in Importance')\n",
    "plt.title('Features with Highest Variance in Importance Across Classes')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'feature_importance_variance.png'), dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Save importance scores\n",
    "importance_df.to_csv(os.path.join(output_dir, 'feature_importance_scores.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cf613d4-0527-40e5-a054-aea540f4547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats = {\n",
    "    'total_samples_analyzed': len(X_data),\n",
    "    'samples_used_for_importance': min(2000, len(X_data)),\n",
    "    'total_features': len(feature_names),\n",
    "    'sequence_length': X_data.shape[1],\n",
    "    'model_accuracy': np.mean(pred_classes == y_subset),\n",
    "    'mean_confidence': np.mean(max_probs),\n",
    "    'top_10_features': mean_importance.nlargest(10).to_dict(),\n",
    "    'class_distribution': {str(le.inverse_transform([i])[0]): int(count) \n",
    "                          for i, count in zip(unique_classes, class_counts)}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88a89450-a3f5-4151-be7d-9f2815bafa1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Important Features:\n",
      " 1. 397_24: 0.0224\n",
      " 2. 397_0: 0.0148\n",
      " 3. 397_8: 0.0143\n",
      " 4. 397_9: 0.0138\n",
      " 5. 397_25: 0.0126\n",
      " 6. 397_11: 0.0119\n",
      " 7. 291_5: 0.0112\n",
      " 8. 397_30: 0.0104\n",
      " 9. 459_2: 0.0103\n",
      "10. 459_1: 0.0100\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "for i, (feat, score) in enumerate(summary_stats['top_10_features'].items(), 1):\n",
    "    print(f\"{i:2d}. {feat}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py310)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
